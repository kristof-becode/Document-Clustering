{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Kleister-charity dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/becode/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/becode/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/becode/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np                                                        # linear algebra lib\n",
    "import pandas as pd                                                       # dataframes lib\n",
    "import re                                                                 # regex lib\n",
    "import matplotlib.pyplot as plt                                           # plotting lib \n",
    "import nltk                                                               # Nat Lang Processing package\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer   # NLTK tokenization modules\n",
    "from nltk.corpus import stopwords                                         # NLTK stop words  \n",
    "from nltk.stem.wordnet import WordNetLemmatizer                           # NLTK lemmitization lib\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))                              # NLTK stopwords set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in test-A TSV and explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will work on a subset of the full Kleister-charity dataset which contains +1700 documents.\n",
    "\n",
    "More specifically I will look at the 609 test-A documents. The test-A tsv file contains the filename of\n",
    "each document and its respective OCR'ed text. There are 4 different ways that the thext has been OCR'ed, \n",
    "each in a seperate column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               filename  \\\n",
      "0  abbf98ed31e28068150dce58296302ee.pdf   \n",
      "1  f3e363848aea2fa645814f2de0221a5a.pdf   \n",
      "2  62acdd1bbd0dfeea27da2720eb795449.pdf   \n",
      "3  e734bc7dfc9b37c5dd2c3a37693062e8.pdf   \n",
      "4  cb6b0949a2f9294750e436f7ea2f10ce.pdf   \n",
      "5  87c977ccb9bdf111b1397e9c4ada2470.pdf   \n",
      "6  39df988309a04c631445b04ebd6a4a53.pdf   \n",
      "7  e38bd1524e145b49edf991ab8f3e153d.pdf   \n",
      "8  bb80583ada5875ccb4690ffa22f97bab.pdf   \n",
      "9  7ae3665305caf119acabb0863ea1e46d.pdf   \n",
      "\n",
      "                                                keys  \\\n",
      "0  address__post_town address__postcode address__...   \n",
      "1  address__post_town address__postcode address__...   \n",
      "2  address__post_town address__postcode address__...   \n",
      "3  address__post_town address__postcode address__...   \n",
      "4  address__post_town address__postcode address__...   \n",
      "5  address__post_town address__postcode address__...   \n",
      "6  address__post_town address__postcode address__...   \n",
      "7  address__post_town address__postcode address__...   \n",
      "8  address__post_town address__postcode address__...   \n",
      "9  address__post_town address__postcode address__...   \n",
      "\n",
      "                                           text_djvu  \\\n",
      "0  PRAESTAT OPES SAPIENTIA\\nHAMPTON SCHOOL\\n(A Ch...   \n",
      "1  28 November 2015\\nHPPC Financial Statements YE...   \n",
      "2  1\\nIER\\nMarch 2012\\nIndependent Examiner’s\\nRe...   \n",
      "3  ~e-SCh44\\nBreachwood Green PreSchool\\nRegister...   \n",
      "4  hahcrar alcrlfcrd haptat church\\nBishop's Stor...   \n",
      "5  \\f1 May\\nLITTLE BEARS PRE SCHOOL\\nBALANCE SHEE...   \n",
      "6  Wirral Chinese Association\\n25th Annual Genera...   \n",
      "7  Charity number: 113425tt\\nSt John of Jerusalem...   \n",
      "8  Blundell’s School\\nAnnual report and\\nconsolid...   \n",
      "9  WHITCHURCH COMMUNITY SERVICES ASSOCIATION\\nCha...   \n",
      "\n",
      "                                      text_tesseract  \\\n",
      "0  \\n0 r = - ’ AT \\ PRAESTAT OPES SAPIENTIA HAMPT...   \n",
      "1  Charity Registration No. 1076498\\nCompany Regi...   \n",
      "2  LU O  \\nSection A\\nReport to the trustees/memb...   \n",
      "3  \\n \\no \\\\nPe - scno®\\nBreachwood Green Pre-Sch...   \n",
      "4  5\\nbishops stortford baptist church\\nBishop’s ...   \n",
      "5  Company Registration No. 07557678 (England and...   \n",
      "6  Wirral Chinese Association\\n25th Annual Genera...   \n",
      "7  Charlty number: 1134256\\nSt John of Jerusalem ...   \n",
      "8  Blundell’s School\\nAnnual report and\\nconsolid...   \n",
      "9  WHITCHURCH COMMUNITY SERVICES ASSOCIATION\\nCha...   \n",
      "\n",
      "                                       text_textract  \\\n",
      "0  PRAESTAT OPES SAPIENTIA\\nHAMPTON SCHOOL\\n(A Ch...   \n",
      "1  Charity Registration No. 1076498\\nCompany Regi...   \n",
      "2  CHARITY\\nIndependent Examiner's\\nCOMMISSION\\nR...   \n",
      "3  eochwood\\nA\\ne-schoo\\nBreachwood Green Pre-Sch...   \n",
      "4  bishops stortford baptist church\\nBishop's Sto...   \n",
      "5  Company Registration No. 07557678 (England and...   \n",
      "6  Wirral Chinese Association\\n25th Annual Genera...   \n",
      "7  Charity number: 1134256\\nSt John of Jerusalem ...   \n",
      "8  Blundell's School\\nAnnual report and\\nconsolid...   \n",
      "9  WHITCHURCH COMMUNITY SERVICES ASSOCIATION\\nCha...   \n",
      "\n",
      "                                           text_best  \n",
      "0  PRAESTAT OPES SAPIENTIA\\nHAMPTON SCHOOL\\n(A Ch...  \n",
      "1  28 November 2015\\nHPPC Financial Statements YE...  \n",
      "2  1\\nIER\\nMarch 2012\\nIndependent Examiner’s\\nRe...  \n",
      "3  ~e-SCh44\\nBreachwood Green PreSchool\\nRegister...  \n",
      "4  hahcrar alcrlfcrd haptat church\\nBishop's Stor...  \n",
      "5  Company Registration No. 07557678 (England and...  \n",
      "6  Wirral Chinese Association\\n25th Annual Genera...  \n",
      "7  Charity number: 113425tt\\nSt John of Jerusalem...  \n",
      "8  Blundell’s School\\nAnnual report and\\nconsolid...  \n",
      "9  WHITCHURCH COMMUNITY SERVICES ASSOCIATION\\nCha...   \n",
      "\n",
      "(609, 6)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 609 entries, 0 to 608\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   filename        609 non-null    object\n",
      " 1   keys            609 non-null    object\n",
      " 2   text_djvu       592 non-null    object\n",
      " 3   text_tesseract  609 non-null    object\n",
      " 4   text_textract   609 non-null    object\n",
      " 5   text_best       609 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 28.7+ KB\n",
      "None\n",
      "filename           0\n",
      "keys               0\n",
      "text_djvu         17\n",
      "text_tesseract     0\n",
      "text_textract      0\n",
      "text_best          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# read Kleister-charity test-A tsv file into dataframe\n",
    "kl = pd.read_csv('/home/becode/AI/Data/Faktion/kleister-charity/test-A/in.tsv', sep='\\t',\n",
    "            names=['filename', 'keys', 'text_djvu', 'text_tesseract', 'text_textract', 'text_best'])\n",
    "print(kl.head(10),\"\\n\")\n",
    "print(kl.shape)\n",
    "print(kl.info())\n",
    "print(kl.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In total we have 609 filenames with OCR'ed text in 4 different OCR text columns\n",
    "\n",
    "OCR 'text_djvu' has 17 missing document texts so we'll probably not use that one. We can already drop the 'keys' column as this is not needed for our purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop not needed 'keys' column\n",
    "kl = kl.drop(columns='keys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare different OCR columns; 'text_djvu','text_textract','text_best', 'text_tesseract'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"{kl.loc[0,'text_djvu']}\\n\")\n",
    "print(f\"{kl.loc[0,'text_tesseract']}\\n\")\n",
    "print(f\"{kl.loc[0,'text_textract']}\\n\")\n",
    "print(kl.loc[0,'text_best'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"{kl.loc[100,'text_djvu']}\\n\")\n",
    "print(f\"{kl.loc[100,'text_tesseract']}\\n\")\n",
    "print(f\"{kl.loc[100,'text_textract']}\\n\")\n",
    "print(kl.loc[100,'text_best'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"{kl.loc[300,'text_djvu']}\\n\")\n",
    "print(f\"{kl.loc[300,'text_tesseract']}\\n\")\n",
    "print(f\"{kl.loc[300,'text_textract']}\\n\")\n",
    "print(kl.loc[300,'text_best'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select 'text_tesseract' OCR column and preprocess text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All 4 OCR columns appear to have their pros and cons. \n",
    "I will make the arbitrary choice to keep the 'text_tesseract' to work with and drop the other OCR columns.\n",
    "From looking at the OCR content of the documents we can already clean it up a bit by removing line breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only 'text_tesseract' OCR text column; the OCR columns all have pros and cons\n",
    "kl = kl.drop(columns=['text_djvu','text_textract','text_best'])\n",
    "\n",
    "# some easy preprocessing after looking at text content; remove '\\n' and '\\\\n'\n",
    "kl['text_tesseract'] = kl['text_tesseract'].astype(str)\n",
    "kl['text_tesseract'] = kl['text_tesseract'].apply(lambda x: x.replace(\"\\n\",\" \"))\n",
    "kl['text_tesseract'] = kl['text_tesseract'].apply(lambda x: x.replace(\"\\\\n\",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check result of first preprocessing\n",
    "print(f\"{kl.loc[0,'text_tesseract']}\\n\")\n",
    "print(f\"{kl.loc[100,'text_tesseract']}\\n\")\n",
    "print(kl.loc[300,'text_tesseract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To cluster the documents we will need to tokenize and process the text content of the documents and vectorise the tokens using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize or more specifally lemmitize sentence and word\n",
    "def tokenize_lem(text):\n",
    "    lem = WordNetLemmatizer()\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.match('[a-zA-Z]', token) and not token in stop_words and len(token)>3: # changed from if re.search('[a-zA-Z]', token)\n",
    "            lemmatized_word = lem.lemmatize(token)\n",
    "            filtered_tokens.append(lemmatized_word)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function to 'text_tesseract' column\n",
    "kl['text_tesseract'] = kl['text_tesseract'].apply(tokenize_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['praestat', 'opes', 'sapientia', 'hampton', 'school', 'charitable', 'company', 'limited', 'guarantee', 'report', 'financial', 'statement', 'year', 'ended', 'august', 'registered', 'company', 'registered', 'charity', 'hampton', 'school', 'content', 'page', 'chairman', 'report', 'legal', 'administrative', 'information', 'governor', 'report', 'independent', 'auditor', 'report', 'statement', 'financial', 'activity', 'year', 'ended', 'august', 'statement', 'financial', 'activity', 'year', 'ended', 'august', 'balance', 'sheet', 'cashflow', 'statement', 'note', 'financial', 'statement', 'hampton', 'school', 'chairman', 'report', 'year', 'ended', 'august', 'delighted', 'another', 'successful', 'year', 'trust', 'school', 'success', 'publicly', 'recognised', 'team', 'independent', 'school', 'inspectorate', 'inspected', 'school', 'march', 'report', 'highlighted', 'many', 'excellent', 'area', 'school', 'activity', 'particularly', 'concluded', 'hampton', 'school', 'quality', 'pupil', 'achievement', 'academic', 'co-curricular', 'area', 'exceptional', 'went', 'declare', 'quality', 'leadership', 'management', 'including', 'link']\n"
     ]
    }
   ],
   "source": [
    "# check first 1OO tokenized words in 'text_tesseract' column\n",
    "print(kl['text_tesseract'][0][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check result of tokenization for different documents\n",
    "print(kl['text_tesseract'][0])\n",
    "print(kl['text_tesseract'][0])\n",
    "print(kl['text_tesseract'][100])\n",
    "print(kl['text_tesseract'][300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47     63100\n",
      "429    42752\n",
      "375    40201\n",
      "56     22874\n",
      "21     14683\n",
      "       ...  \n",
      "576      322\n",
      "235      322\n",
      "317      291\n",
      "260      150\n",
      "390      119\n",
      "Name: text_tesseract, Length: 609, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check length of tokens lists per document\n",
    "print(kl['text_tesseract'].apply(lambda x : len(x)).sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429    7225\n",
      "375    6872\n",
      "47     6720\n",
      "21     3305\n",
      "528    3298\n",
      "       ... \n",
      "576     167\n",
      "235     166\n",
      "317     151\n",
      "260     118\n",
      "390      95\n",
      "Name: text_tesseract, Length: 609, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check unique tokenized words per document\n",
    "print(kl['text_tesseract'].apply(lambda x : len(set(x))).sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we will vectorise the tokenized text with Sci-kit Learn's TF-IDF. See next notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
